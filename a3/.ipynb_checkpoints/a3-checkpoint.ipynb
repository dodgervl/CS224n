{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 45]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [3,4,5,51,45]\n",
    "a.pop(-2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "parser_transitions.py: Algorithms for completing partial parsess.\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
    "                                        Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
    "        self.sentence = sentence\n",
    "        ### YOUR CODE HERE (3 Lines)\n",
    "        self.stack = ['ROOT']\n",
    "        if sentence is str: self.buffer = sentence.split()\n",
    "        else: self.buffer = list(sentence)\n",
    "        self.dependencies = []\n",
    "        ### Your code should initialize the following fields:\n",
    "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
    "        ###                 last element of the list.\n",
    "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
    "        ###                  buffer as the first item of the list\n",
    "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "        ###             tuples where each tuple is of the form (head, dependent).\n",
    "        ###             Order for this list doesn't matter.\n",
    "        ###\n",
    "        ### Note: The root token should be represented with the string \"ROOT\"\n",
    "        ###\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
    "                                left-arc, and right-arc transitions. You can assume the provided\n",
    "                                transition is a legal transition.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~7-10 Lines)\n",
    "        if transition == 'S':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'LA':\n",
    "            self.dependencies.append((self.stack[-1],self.stack.pop(-2)))\n",
    "        elif transition == 'RA':\n",
    "            self.dependencies.append((self.stack[-2],self.stack.pop(-1))) \n",
    "        ### TODO:\n",
    "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
    "        ###     described in the pdf handout:\n",
    "        ###         1. Shift\n",
    "        ###         2. Left Arc\n",
    "        ###         3. Right Arc\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
    "\n",
    "        @return dsependencies (list of string tuples): The list of dependencies produced when\n",
    "                                                        parsing the sentence. Represented as a list of\n",
    "                                                        tuples where each tuple is of the form (head, dependent).\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    @param sentences (list of list of str): A list of sentences to be parsed\n",
    "                                            (each sentence is a list of words and each word is of type string)\n",
    "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
    "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "                                returns a list of transitions predicted for each parse. That is, after calling\n",
    "                                    transitions = model.predict(partial_parses)\n",
    "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
    "\n",
    "\n",
    "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
    "                                                    list for a parsed sentence. Ordering should be the\n",
    "                                                    same as in sentences (i.e., dependencies[i] should\n",
    "                                                    contain the parse for sentences[i]).\n",
    "    \n",
    "    \"\"\"\n",
    "    dependencies = []\n",
    "    ### YOUR CODE HERE (~8-10 Lines)\n",
    "    unfinished_parses = []\n",
    "    for s in sentences: unfinished_parses.append(PartialParse(s))\n",
    "    while unfinished_parses:\n",
    "        transitions = model.predict(unfinished_parses[0:batch_size])\n",
    "        i=0\n",
    "        while i < len(transitions):\n",
    "            unfinished_parses[i].parse_step(transitions[i])\n",
    "            if len(unfinished_parses[i].stack) == 1 and not unfinished_parses[i].buffer:\n",
    "                dependencies.append(unfinished_parses[i].dependencies) \n",
    "                unfinished_parses.pop(i)\n",
    "                transitions.pop(i)\n",
    "                i-=1\n",
    "            i+=1\n",
    "    ### TODO:\n",
    "    ###     Implement the minibatch parse algorithm as described in the pdf handout\n",
    "    ###\n",
    "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
    "    ###                 unfinished_parses = partial_parses[:].\n",
    "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
    "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
    "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
    "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
    "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
    "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
    "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "             [\"left\", \"arcs\", \"only\"],\n",
    "             [\"left\", \"arcs\", \"only\", \"again\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PartialParse object at 0x7fbe183cc7b8>\n",
      "<__main__.PartialParse object at 0x7fbe183cce10>\n",
      "<__main__.PartialParse object at 0x7fbe183cce80>\n",
      "<__main__.PartialParse object at 0x7fbe183cc0f0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.PartialParse at 0x7fbe183cc0f0>,\n",
       " <__main__.PartialParse at 0x7fbe183cce80>,\n",
       " <__main__.PartialParse at 0x7fbe183cce10>,\n",
       " <__main__.PartialParse at 0x7fbe183cc7b8>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_parses = [PartialParse(s) for s in sentences]\n",
    "#unfinished_parses = partial_parses.copy()\n",
    "#unfinished_parses = list(partial_parses)\n",
    "while unfinished_parses:\n",
    "    print(unfinished_parses.pop())\n",
    "\n",
    "partial_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ROOT']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_parses[0].stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.PartialParse at 0x7fbe183d12b0>,\n",
       " <__main__.PartialParse at 0x7fbe183d1a20>,\n",
       " <__main__.PartialParse at 0x7fbe183d1f28>,\n",
       " <__main__.PartialParse at 0x7fbe183d1dd8>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/vova/DATA/Ubuntu_Applications/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py', 'part_c', '/run/user/1000/jupyter/kernel-069788f6-ff92-48e1-acf8-c8fa1c54f3a1.json', 'part_c']\n",
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")\n",
    "\n",
    "\n",
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print(\"minibatch_parse test passed!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = sys.argv\n",
    "    print(args)\n",
    "    if len(args) != 4:\n",
    "        raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
    "    elif args[1] == \"part_c\":\n",
    "        test_parse_step()\n",
    "        test_parse()\n",
    "    elif args[1] == \"part_d\":\n",
    "        test_minibatch_parse()\n",
    "    else:\n",
    "        raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### YOUR CODE HERE (~5 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###         It has been shown empirically, that this provides better initial weights\n",
    "        ###         for training networks than random uniform initialization.\n",
    "        ###         For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
    "        ### Hints:\n",
    "        ###     - After you create a linear layer you can access the weight\n",
    "        ###       matrix via:\n",
    "        ###         linear_layer.weight\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ###  YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `nn.init.xavier_uniform_()` not found.\n"
     ]
    }
   ],
   "source": [
    "?nn.init.xavier_uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8335, -0.0691, -0.3709],\n",
      "         [ 0.7279, -0.3274,  0.1300],\n",
      "         [ 0.5178, -0.8550, -0.7663],\n",
      "         [ 1.5077, -1.3409,  0.4456]],\n",
      "\n",
      "        [[ 0.5178, -0.8550, -0.7663],\n",
      "         [ 2.9806,  0.4044,  1.9008],\n",
      "         [ 0.7279, -0.3274,  0.1300],\n",
      "         [ 2.9806,  0.4044,  1.9008]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(6, 3)\n",
    "# a batch of 2 samples of 4 indices eac\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,3]])\n",
    "a = embedding(input)\n",
    "print(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 4, 5],\n",
       "        [4, 3, 2, 9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
